---
title: "Applications of Item Response Theory in R"
author: "W. Jake Thompson, Ph.D."
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: true
    toc_float: true
    number_sections: false
    css: styles.css
bibliography: ["bib/refs.bib", "bib/packages.bib"]
biblio-style: apa
csl: csl/apa.csl
link-citations: yes
---

```{r setup, include = FALSE}
library(tidyverse)
library(readxl)
library(mirt)

library(furrr)

library(htmlwidgets)
library(slickR)
library(svglite)

library(here)
library(glue)

knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "c",
  out.width = "90%",
  fig.retina = 3
)
```

```{r functions, include = FALSE}
inv_logit <- function(x) {
  1 / (1 + exp(-x))
}
int_runs <- function(run, sep = ",", collapse = ", ") {
  run <- as.numeric(flatten(str_split(run, sep)))
  
  rundiff <- c(1, diff(run))
  difflist <- split(run, cumsum(rundiff != 1))
  all_runs <- unlist(lapply(difflist, function(x) {
    if (length(x) %in% 1:2) {
      as.character(x) 
    } else {
      paste0(x[1], "-", x[length(x)])
    }
  }), use.names = FALSE)
  paste(all_runs, collapse = collapse)
}
icc_calc <- function(theta, itemid, a, b, c, u, .pb = NULL) {
  if ((!is.null(.pb)) && inherits(.pb, "Progress") && (.pb$i < .pb$n)) {
    .pb$tick()$print()
  }
  
  dat <- tibble(theta = theta, itemid = itemid, a = a, b = b, c = c, u = u)
  
  icc <- dat %>%
    unnest(cols = b) %>%
    mutate(exp_score = c + (1 - c) * inv_logit(a * (theta - b))) %>%
    add_row(exp_score = 1, .before = 1) %>%
    add_row(exp_score = 0) %>%
    rowid_to_column(var = "cat") %>%
    mutate(cat = cat - 1,
           theta = mean(theta, na.rm = TRUE),
           itemid = unique(itemid[!is.na(itemid)]),
           a = mean(a, na.rm = TRUE),
           c = mean(c, na.rm = TRUE),
           u = mean(u, na.rm = TRUE),
           p = exp_score - lead(exp_score, n = 1, default = 0)) %>%
    select(itemid, theta, cat, everything())

  return(icc)
}
icc_plot <- function(itemid, itemtype, NIS, maxscore, icc_data) {
  if (NIS == 2) {
    icc_data <- filter(icc_data, !(cat %in% c(0, 2)))
    plot <- ggplot(icc_data, aes(x = theta, y = p))
  } else {
    icc_data <- filter(icc_data, cat < NIS)
    plot <- ggplot(icc_data, aes(x = theta, y = p, color = factor(cat)))
  }
  
  plot <- plot +
    geom_line(size = 2) +
    scale_x_continuous(breaks = seq(-3, 3, by = 1)) +
    expand_limits(y = c(0, 1)) +
    labs(x = expression(theta),
         y = expression(paste("P(X >= x | ", theta, ")")),
         title = "Item Characteristic Curve",
         subtitle = itemid,
         color = "Score") +
    theme_bw() +
    theme(legend.position = "bottom")
  
  return(plot)
}
```

This document includes a more complete example of conducting an item response theory (IRT) analysis in R. This work-up includes not only estimation of item and person parameters, but also the creation of item characteristic curves, test information, reliability, and model fit. Most data cleaning and manipulation will utilize a suite a packages known as the **tidyverse** [@tidyverse2019]. The IRT analysis will use the **mirt** package [@mirt2012].


## Data Cleaning

The first step of any analysis is to read in the data. For this example, we will use a balanced sample of males and females from a large scale operational assessment. This data set contains 5,000 respondents to 40 items assessing the respondents' knowledge and understandings of engineering design. Also included is a file of metadata, describing the 40 items.

```{r read-data}
library(tidyverse)
library(readxl)

ied <- read_csv(here("data", "IED_data.csv"),
                col_types = cols(.default = col_integer(),
                                 gender = col_character()))
meta <- read_excel(here("data", "metadata.xlsx"), sheet = "IED")
```

We next have to determine how each item will be modeled. To do this, we create a new variable called `mirt_type`. When the items are dichotomously scored (i.e., `NIS == 2`), we will use the 3-parameter logistic model [3PL; @birnbaum_1968]. For polytomously scored items, we will use the graded response model [GRM; @samejima_1969; @samejima_1972; @samejima_1997]. The modeling will actually happen with the **mirt** package [@R-mirt]. For a list of available models, see [`?mirt()`](https://rdrr.io/cran/mirt/man/mirt.html).

We also need to clean the response data. In the `ied` data, missing values have been coded a `-9`. The `na_if` function can be used to replace a given value with R's internal representation of missing values, `NA`.

```{r clean-data}
clean_meta <- meta %>%
  select(itemid, itemtype, NIS, maxscore) %>%
  mutate(mirt_type = case_when(NIS == 2 ~ "3PL",
                               NIS > 2 ~ "graded"))

clean_ied <- ied %>%
  mutate_all(~na_if(.x, -9))
```

## Estimate IRT Model

To estimate the IRT model, we'll use the **mirt** package [@R-mirt]. This function requires that the data include only item responses, so we'll create a data set, `model_data`, that is the same as the original data but with the `studentid` and `gender` columns removed.

```{r mirt-data}
model_data <- clean_ied %>%
  select(-studentid, -gender)
```

We are now ready to estimate the model. The first argument is the `data`, which we specify as the `model_data` we just created. Next, the actual model must be specified. Because we are using a unidimensional model, we have only one factor, called `F1`. This factor is measured by items 1 through the number of columns in our `model_data`. We use the `glue()` function to dynamically determine the number of items. In this example, `glue("F1 = 1-{ncol(model_data)}")` will evaluate to ``"`r glue("F1 = 1-{ncol(model_data)}")`"``. Then, for each item, we specify what the item type is. We calculated this when we created the `clean_meta` data, so we can pull that variable out. Note that this assumes the the metadata is in the same order as the columns of `model_data`. Finally, we'll set a random seed to make sure we all get the same results (they should be pretty close without this).

```{r fit-mirt, results = "hide", cache = TRUE}
library(mirt)

model_3pl <- mirt(data = model_data, model = glue("F1 = 1-{ncol(model_data)}"),
                  itemtype = clean_meta$mirt_type,
                  technical = list(set.seed = 9416))
```

Now we've estimated the model!

```{r show-model}
model_3pl
```


### IRT Parameters

The default output is not incredibly useful. What we ultimately want are the estimated item and person parameters. We'll focus first on item parameters.

#### Item Parameters

We can extract the item parameters by the using the `coef()` function. For each item, we see the slope (`a1`) and intercepts (`d`, `d1`, `d2`, ...).

```{r default-coef}
coef(model_3pl)
```

Often, it is more useful to think about the parameters using the more well known $a$, $b$, and $c$ parameters. These can be retrieved by setting `IRTpars = TRUE`.

```{r irt-coef}
coef(model_3pl, IRTpars = TRUE)
```

The last problem to solve is that the coefficients are returned in a list format. This is done because not every items has the same set of parameters. Specifically, items will have different numbers of $b$ parameters, depending on how many score categories are present. However, with some **tidyverse** magic, we can create a data frame that has one row per item, with all of the $b$ parameters nested together.

```{r item-params}
item_params <- coef(model_3pl, IRTpars = TRUE) %>%
  list_modify(GroupPars = zap()) %>%
  imap_dfr(function(x, y) {
    as_tibble(x) %>%
      add_column(itemid = y, .before = 1) %>%
      nest(b = starts_with("b")) %>%
      mutate(b = map(b, function(z) {
        z %>%
          pivot_longer(cols = everything(), names_to = "param",
                       values_to = "value") %>%
          deframe()
      })) %>%
      select(itemid, a, b, everything())
  }) %>%
  rename(c = g) %>%
  replace_na(list(c = 0, u = 1))

item_params
```

#### Person Parameters

We are also likely interested in the person parameters, or the respondent ability estimates. We can extract the ability estimates using the `fscores()` function. When then do some formatting to get the scores into a nice data frame, and add a `studentid` column so we can keep track of which ability estimate goes with each respondent.

```{r person-params}
person_params <- fscores(model_3pl) %>%
  as_tibble(.name_repair = ~"theta") %>%
  rowid_to_column(var = "studentid")

person_params
```

## Test Characteristics

To explore the assessment further, we will examine the item characteristic curves. For dichotomous items, these plots display the probability of providing a correct response, across the range of ability. For polytomous items, these plots show, across the range of ability, the probability of scoring in each category. To create the plots, we define range of ability we are interested in, and then calculate the probabilities at each ability level using the custom `icc_calc()` function.

```{r iccs, warning = FALSE, results = "hide", cache = TRUE}
plan(multiprocess)

iccs <- crossing(theta = seq(-3, 3, by = 0.01),
                 itemid = colnames(model_data)) %>%
  left_join(item_params, by = "itemid") %>%
  future_pmap_dfr(icc_calc, .progress = TRUE) %>%
  nest(icc_data = -c(itemid))
```

```{r display-icc, echo = FALSE}
icc_plots <- meta %>%
  select(itemid, itemtype, NIS, maxscore) %>%
  left_join(mutate(iccs, itemid = as.double(itemid)), by = "itemid") %>%
  pmap(icc_plot)

plots_svg <- map(icc_plots, function(x) {
  xmlSVG({show(x)}, standalone = TRUE)
})

paging <- JS("function(slick,index) {
                return '<a>'+(dotObj[index])+'</a>';
              }")
s2 <- htmltools::tags$script(
  sprintf("var dotObj = %s", jsonlite::toJSON(clean_meta$itemid))
)
opts_dot_id <- settings(
    initialSlide = 0,
    slidesToShow = 1,
    focusOnSelect = TRUE,
    dots = TRUE,
    customPaging = paging
)

slick_dots_id <- slickR(
  obj = plots_svg,
  height = 500,
  width = "95%"
) + opts_dot_id

htmltools::browsable(htmltools::tagList(s2, slick_dots_id))
```


```{r test-info, cache = TRUE}
info_calc <- function(theta, itemid, a, b, c, u, .pb = NULL) {
  if ((!is.null(.pb)) && inherits(.pb, "Progress") && (.pb$i < .pb$n)) {
    .pb$tick()$print()
  }
  
  dat <- tibble(theta = theta, itemid = itemid, a = a, b = b, c = c, u = u)
  
  info <- dat %>%
    unnest(cols = b) %>%
    mutate(exp_score = c + (1 - c) * inv_logit(a * (theta - b))) %>%
    add_row(exp_score = 1, .before = 1) %>%
    add_row(exp_score = 0) %>%
    rowid_to_column(var = "cat") %>%
    mutate(cat = cat - 1,
           a = mean(a, na.rm = TRUE),
           p = exp_score - lead(exp_score, 1),
           a1 = ((a^2) * (exp_score * (1 - exp_score) - lead(exp_score, 1) * (1 - lead(exp_score, 1)))^ 2) / p) %>%
    pull(a1) %>%
    sum(na.rm = TRUE)
  
  dat %>%
    mutate(info = info)
}

plan(multiprocess)

info <- crossing(theta = seq(-3, 3, by = 0.01),
                      itemid = colnames(model_data)) %>%
  left_join(item_params, by = "itemid") %>%
  future_pmap_dfr(info_calc, .progress = TRUE)

test_info <- info %>%
  group_by(theta) %>%
  summarize(info = sum(info)) %>%
  mutate(se = 1 / sqrt(info))

ggplot(test_info, aes(x = theta, y = info)) +
  geom_line() +
  scale_x_continuous(breaks = seq(-3, 3, by = 1)) +
  labs(x = expression(theta), y = "Information", title = "Test Information") +
  theme_bw()

ggplot(test_info, aes(x = theta, y = se)) +
  geom_line() +
  scale_x_continuous(breaks = seq(-3, 3, by = 1)) +
  expand_limits(y = 0) +
  labs(x = expression(theta), y = "Standard Error",
       title = "Conditional Standard Error of Measurement") +
  theme_bw()
```

## Model Fit

```{r expected-scores, eval = FALSE}
expected_calc <- function(studentid, gender, itemid, obs_score, theta, a, b, c,
                          u, .pb = NULL) {
  if ((!is.null(.pb)) && inherits(.pb, "Progress") && (.pb$i < .pb$n)) {
    .pb$tick()$print()
  }
  
  dat <- tibble(studentid = studentid, gender = gender, itemid = itemid,
                obs_score = obs_score, theta = theta, a = a, b = b, c = c,
                u = u)
  exp_score <- dat %>%
    unnest(cols = b) %>%
    mutate(exp_score = c + (1 - c) * inv_logit(a * (theta - b))) %>%
    add_row(exp_score = 1, .before = 1) %>%
    add_row(exp_score = 0) %>%
    rowid_to_column(var = "cat") %>%
    mutate(cat = cat - 1,
           p = exp_score - lead(exp_score, 1),
           exp = cat * p) %>%
    pull(exp) %>%
    sum(na.rm = TRUE)
  
  dat %>%
    mutate(exp_score = exp_score,
           residual = obs_score - exp_score)
}

plan(multiprocess)

residuals <- clean_ied %>%
  pivot_longer(cols = -c(studentid, gender),
               names_to = "itemid", values_to = "obs_score") %>%
  filter(!is.na(obs_score)) %>%
  left_join(person_params, by = "studentid") %>%
  left_join(item_params, by = "itemid") %>%
  future_pmap_dfr(expected_calc, .progress = TRUE) %>%
  select(studentid, gender, itemid, obs_score, exp_score, residual)
```


## References {-}

```{r write-packages, include = FALSE}
if (!file.exists("bib/packages.bib")) file.create("bib/packages.bib")
suppressWarnings(
  knitr::write_bib(c(.packages()), "bib/packages.bib")
)

# Correct capitalization in packages
read_lines("bib/packages.bib") %>%
  str_replace_all("mirt:", "{mirt}:") %>%
  str_replace_all(" Stan", " {Stan}") %>%
  str_replace_all("rstan:", "{RStan}:") %>%
  str_replace_all("rstanarm:", "{RStanArm}:") %>%
  str_replace_all("Bayesian", "{Bayesian}") %>%
  str_replace_all("loo:", "{loo}:") %>%
  str_replace_all("WAIC", "{WAIC}") %>%
  write_lines("bib/packages.bib")
```

<div id="refs"></div>
